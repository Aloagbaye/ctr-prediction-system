# CTR Prediction System

A complete end-to-end machine learning system for predicting Click-Through Rate (CTR) of ad impressions. This project covers data generation, feature engineering, model training, evaluation, and production deployment.

## ğŸ¯ Project Status

- âœ… **Phase 1**: Data Preparation & Exploration
- âœ… **Phase 2**: Feature Engineering
- âœ… **Phase 3**: Baseline Model Development
- â³ Phase 4: Model Optimization
- â³ Phase 5: Model Evaluation
- âœ… **Phase 6**: API Development (Current)
- â³ Phase 7: Containerization
- â³ Phase 8: Cloud Deployment
- â³ Phase 9: Online Evaluation & Monitoring

## ğŸ“‹ Phase 1: Data Preparation & Exploration

### Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### Option A: Use Simulated Data (Quick Start)

2. **Generate simulated data:**
   ```bash
   python scripts/generate_data.py --n-impressions 100000 --output data/raw/impressions.csv
   ```

   Or use custom parameters:
   ```bash
   python scripts/generate_data.py \
     --n-users 10000 \
     --n-ads 1000 \
     --n-impressions 100000 \
     --output data/raw/impressions.csv \
     --seed 42
   ```

3. **Run exploratory data analysis:**
   ```bash
   python scripts/explore_data.py --input data/raw/impressions.csv --output-dir data/eda
   ```

### Option B: Use Real Kaggle Avazu Data (Recommended for Production)

2. **Set up Kaggle API credentials:**
   - Go to https://www.kaggle.com/account
   - Scroll to 'API' section and click 'Create New API Token'
   - Place `kaggle.json` in `~/.kaggle/` (Linux/Mac) or `C:\Users\<username>\.kaggle\` (Windows)
   - Set permissions: `chmod 600 ~/.kaggle/kaggle.json` (Linux/Mac)

3. **Download Avazu dataset:**
   ```bash
   python scripts/download_avazu.py --download
   ```

4. **Load and process Avazu data:**
   ```bash
   # Load a sample (10% of data - recommended for initial exploration)
   python scripts/download_avazu.py --load --sample-frac 0.1 --output data/raw/avazu_processed.csv
   
   # Or load specific number of rows (for testing)
   python scripts/download_avazu.py --load --n-rows 100000 --output data/raw/avazu_processed.csv
   ```

5. **Run exploratory data analysis:**
   ```bash
   python scripts/explore_data.py --input data/raw/avazu_processed.csv --output-dir data/eda
   ```

### Dataset Structure

The generated dataset contains the following columns:

- `user_id`: Unique user identifier
- `ad_id`: Unique advertisement identifier
- `device`: Device type (mobile, desktop, tablet)
- `placement`: Ad placement location (header, sidebar, footer, in_content, popup)
- `timestamp`: Timestamp of the impression
- `hour`: Hour of day (0-23)
- `day_of_week`: Day of week (0=Monday, 6=Sunday)
- `is_weekend`: Binary indicator (0 or 1)
- `date`: Date of impression
- `clicked`: Target variable (0 or 1)

### EDA Outputs

The EDA script generates:
- Class imbalance analysis
- Temporal patterns (hourly, daily)
- Device and placement CTR analysis
- Feature distributions
- All visualizations saved to `data/eda/`

### Project Structure

```
ctr-prediction-system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw data files
â”‚   â”‚   â””â”€â”€ avazu/        # Avazu dataset (from Kaggle)
â”‚   â”œâ”€â”€ processed/        # Processed features
â”‚   â””â”€â”€ eda/              # EDA plots and reports
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ generator.py      # Simulated data generation
â”‚   â”‚   â”œâ”€â”€ kaggle_loader.py  # Kaggle Avazu data loader
â”‚   â”‚   â””â”€â”€ explore.py        # EDA analysis
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ basic_features.py    # Basic feature engineering
â”‚   â”‚   â”œâ”€â”€ advanced_features.py # Advanced encoding features
â”‚   â”‚   â”œâ”€â”€ feature_selector.py  # Feature selection
â”‚   â”‚   â””â”€â”€ pipeline.py          # Complete pipeline
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ trainer.py        # Model training
â”‚   â”‚   â””â”€â”€ evaluator.py      # Model evaluation
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py           # FastAPI application
â”‚       â”œâ”€â”€ models.py         # Pydantic request/response models
â”‚       â””â”€â”€ predictor.py      # Model prediction logic
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py     # Simulated data generation script
â”‚   â”œâ”€â”€ download_avazu.py    # Avazu dataset downloader
â”‚   â”œâ”€â”€ explore_data.py      # EDA script
â”‚   â”œâ”€â”€ create_features.py   # Feature engineering script
â”‚   â”œâ”€â”€ train_models.py      # Model training script
â”‚   â””â”€â”€ run_api.py           # API server script
â”œâ”€â”€ models/                  # Trained models (generated)
â”œâ”€â”€ config.yaml              # Configuration file
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Using Simulated Data

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Generate data
python scripts/generate_data.py

# 3. Explore data
python scripts/explore_data.py
```

### Using Real Avazu Data

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up Kaggle credentials (see instructions above)

# 3. Download and load Avazu data
python scripts/download_avazu.py --download
python scripts/download_avazu.py --load --sample-frac 0.1

# 4. Explore data
python scripts/explore_data.py --input data/raw/avazu_processed.csv
```

## ğŸ“‹ Phase 2: Feature Engineering

### Create Features

After preparing your data (Phase 1), create engineered features:

```bash
# Create all features (basic + advanced + selection)
python scripts/create_features.py \
  --input data/raw/impressions.csv \
  --output data/processed/features.csv

# Create features with sampling for faster processing (10% of data)
python scripts/create_features.py \
  --input data/raw/avazu_processed.parquet \
  --output data/processed/features.csv \
  --sample-frac 0.1

# Create features with custom options
python scripts/create_features.py \
  --input data/raw/impressions.csv \
  --output data/processed/features.csv \
  --select-by-importance \
  --n-features 50 \
  --sample-frac 0.1
```

### Feature Types Created

- **Historical CTR**: User-level, ad-level, device-level CTR
- **Temporal**: Hour, day, month, business hours, time of day
- **Counts**: Impression counts, time since last click
- **Encoding**: Frequency encoding, target encoding for high-cardinality features
- **Interactions**: DeviceÃ—placement, userÃ—device, hourÃ—placement

## ğŸ“‹ Phase 3: Baseline Model Development

### Train Models

After creating features (Phase 2), train baseline models:

```bash
# Train all models (Logistic Regression, XGBoost, LightGBM)
python scripts/train_models.py \
  --input data/processed/features.csv \
  --output-dir models/

# Train specific models
python scripts/train_models.py --models xgboost lightgbm

# Customize training parameters
python scripts/train_models.py \
  --test-size 0.15 \
  --val-size 0.15
```

### Models Trained

- **Logistic Regression**: Simple baseline, fast and interpretable
- **XGBoost**: High performance, handles non-linear patterns
- **LightGBM**: Fast training, similar performance to XGBoost

### Evaluation Metrics

- **ROC-AUC**: Ability to distinguish between classes
- **Log Loss**: Probability prediction quality
- **PR-AUC**: Better for imbalanced data

## ğŸ“š Documentation

- **[TUTORIAL_PLAN.md](TUTORIAL_PLAN.md)**: Complete tutorial plan covering all phases
- **[Walkthrough Tutorials](walkthrough/)**: Detailed step-by-step guides for each phase
  - [Phase 1: Data Preparation & Exploration](walkthrough/PHASE_1.md) âœ…
  - [Phase 2: Feature Engineering](walkthrough/PHASE_2.md) âœ…
  - [Phase 3: Baseline Model Development](walkthrough/PHASE_3.md) âœ…
  - [Phase 6: API Development](walkthrough/PHASE_6.md) âœ…

## ğŸ›  Technology Stack

- **Python 3.9+**
- **Pandas, NumPy**: Data manipulation
- **Matplotlib, Seaborn**: Visualization
- **Scikit-learn, XGBoost, LightGBM**: Machine learning
- **FastAPI**: API framework (Phase 6+)
- **Docker**: Containerization (Phase 7+)
- **GCP/AWS**: Cloud deployment (Phase 8+)

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.